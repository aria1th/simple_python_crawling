{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:22.661568Z",
     "end_time": "2023-04-19T11:30:22.733569Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import bs4\n",
    "# Tested in Jupyter.\n",
    "\n",
    "!pip install selenium\n",
    "!pip install nest_asyncio\n",
    "!pip install tqdm\n",
    "!pip install aiohttp\n",
    "!pip install bs4\n",
    "\n",
    "# imports and global variables\n",
    "import selenium as se\n",
    "import csv\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    def tqdm(x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SEARCH_BASE_LINK = r'https://kookbang.dema.mil.kr/newsWeb/search.do'\n",
    "# Categories to allow\n",
    "CATEGORIES = {\n",
    "    '국방', '기획연재', '무기백과'\n",
    "}\n",
    "\n",
    "CHOSUN_SEARCH_BASE_LINK = 'https://www.chosun.com/nsearch/?query={kwd}&opt_chk=true&sort=1&siteid=bemil%2Cbemil_news'\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:22.677191Z",
     "end_time": "2023-04-19T11:30:22.733569Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# using selenium to get the page source\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "try:\n",
    "    page = webdriver.Chrome()\n",
    "except:\n",
    "    # Locate the chromedriver.exe in same directory\n",
    "    page = webdriver.Chrome('chromedriver.exe')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:22.692814Z",
     "end_time": "2023-04-19T11:30:26.282279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Debug purpose. Everything will be defined as function in the future"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.237929Z",
     "end_time": "2023-04-19T11:30:26.282279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.249896Z",
     "end_time": "2023-04-19T11:30:26.283278Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.268847Z",
     "end_time": "2023-04-19T11:30:26.283278Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Standard method to inspect, use F12 -> select element -> copy XPATH.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.286274Z",
     "end_time": "2023-04-19T11:30:26.295244Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "class PageManager:\n",
    "    def __init__(self, page: WebDriver):\n",
    "        self.page = page\n",
    "\n",
    "    def get_element_by_xpath(self, xpath):\n",
    "        return self.page.find_element('xpath', xpath)\n",
    "\n",
    "    def get_element_and_click(self, xpath):\n",
    "        elem = self.get_element_by_xpath(xpath)\n",
    "        elem.click()\n",
    "        return elem\n",
    "\n",
    "    def get_element_and_send_keys(self, xpath, keys):\n",
    "        elem = self.get_element_by_xpath(xpath)\n",
    "        elem.send_keys(keys)\n",
    "        return elem\n",
    "\n",
    "    def get_element_by_selector(self, selector):\n",
    "        try:\n",
    "            return self.page.find_element('css selector', selector)\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "\n",
    "class KookbangPageManager(PageManager):\n",
    "\n",
    "    WAIT_TIME = 1\n",
    "    def __init__(self, page: WebDriver):\n",
    "        super().__init__(page)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.page.get(SEARCH_BASE_LINK)\n",
    "\n",
    "    def search(self, keyword: str):\n",
    "        #  input the search word, find #kwd\n",
    "        self.get_element_and_send_keys('//*[@id=\"kwd\"]', keyword)\n",
    "        self.get_element_and_click('//*[@id=\"container\"]/div[2]/div/form/div/div[2]/button')\n",
    "        self.page.implicitly_wait(self.WAIT_TIME)\n",
    "\n",
    "    def find_category_elems(self) -> [WebElement]:\n",
    "        for i in range(9):\n",
    "            selector = f'#container > div.full_search_box > div > ul > li:nth-child({i}) > a'\n",
    "            elem = self.get_element_by_selector(selector)\n",
    "            if elem is not None:\n",
    "                if elem.text in CATEGORIES:\n",
    "                    yield elem\n",
    "    def get_list_of_news_category(self, maxidx:int = -1):\n",
    "        # get next page\n",
    "        # 3 -> 1 page, 4 -> 2 page, ...\n",
    "        # if title is \"다음페이지\" then stop\n",
    "        maxidx = maxidx if maxidx != -1 else 100\n",
    "        for pages in range(maxidx):\n",
    "            if pages != 0:\n",
    "                page_selector = f\"#container > div.full_search_box > div > div.pagination > a:nth-child({pages + 3})\"\n",
    "                # starts from 2 page, until we get to the last page\n",
    "                page_elem = self.get_element_by_selector(page_selector)\n",
    "                if page_elem is None or page_elem.get_attribute('title') == '다음페이지':\n",
    "                    break\n",
    "                page_elem.click()\n",
    "                self.page.implicitly_wait(self.WAIT_TIME)\n",
    "            for idx in range(16): # single page may have 15 news\n",
    "                selector = f'#container > div.full_search_box > div > div.box > ul > li:nth-child({idx}) > a'\n",
    "                elem = self.get_element_by_selector(selector)\n",
    "                if elem is None:\n",
    "                    continue\n",
    "                yield elem\n",
    "        #container > div.full_search_box > div > div.pagination > a:nth-child(4)\n",
    "\n",
    "    def get_news_genexpr(self, keyword : str, maxidx:int = -1):\n",
    "        \"\"\"\n",
    "        Get news generator expression.\n",
    "        :param keyword: Keyword to search\n",
    "        :param maxidx: Maximum number of news to return. -1 for all.\n",
    "        :return: Generator object that returns news link.\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        self.search(keyword)\n",
    "        for categoryClickButtons in self.find_category_elems():\n",
    "            categoryClickButtons.click()\n",
    "            self.page.implicitly_wait(self.WAIT_TIME)\n",
    "            for news in self.get_list_of_news_category(maxidx):\n",
    "                yield news.get_attribute('href')\n",
    "\n",
    "class ChosunMillitaryPageManager(PageManager):\n",
    "    WAIT_TIME = 3\n",
    "    PER_PAGE = 10  # 10 news per page.\n",
    "    BANNED_BBSID = {\n",
    "        10044,\n",
    "        10040,\n",
    "        10129,\n",
    "        10046,\n",
    "        10037\n",
    "    }\n",
    "\n",
    "    def __init__(self, page: WebDriver):\n",
    "        super().__init__(page)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def search(self, keyword: str):\n",
    "        #  input the search word, find #kwd\n",
    "        self.page.get(CHOSUN_SEARCH_BASE_LINK.format(kwd=keyword))\n",
    "        self.page.implicitly_wait(self.WAIT_TIME)\n",
    "        self.page.maximize_window()\n",
    "\n",
    "    def get_page_count(self):\n",
    "        elem = self.get_element_by_selector('#main > div.search-option > div.flex.flex--justify-space-between.flex--align-items-center.box--pad-bottom-sm.box--border.box--border-horizontal.box--border-horizontal-bottom > div:nth-child(1) > p')\n",
    "        text = elem.text\n",
    "        m = re.search(r'(\\d+)건', text)\n",
    "        return int(m.group(1))\n",
    "\n",
    "    def get_page_for_kwd(self, kwd: str, subidx: int):\n",
    "        if subidx == 0:\n",
    "            return\n",
    "        sub_search_page = f'https://www.chosun.com/nsearch/?query={kwd}&page={subidx}&siteid=bemil,bemil_news&sort=1'\n",
    "        self.page.get(sub_search_page)\n",
    "        self.page.implicitly_wait(self.WAIT_TIME)\n",
    "\n",
    "    def get_news_subidx(self, kwd: str, subidx: int):\n",
    "        self.get_page_for_kwd(kwd, subidx)\n",
    "        for idx in range(1, 11):\n",
    "            xpath = f'//*[@id=\"main\"]/div[4]/div[{idx}]/div/div[1]/div[2]/div[1]/div/a'\n",
    "            elem = self.get_element_by_xpath(xpath)\n",
    "            if elem is None:\n",
    "                pass\n",
    "            href = elem.get_attribute('href')\n",
    "            m = re.search(r'bbs_id=(\\d+)', href)\n",
    "            bbsid = int(m.group(1))\n",
    "            if bbsid in self.BANNED_BBSID:\n",
    "                continue\n",
    "            yield href\n",
    "\n",
    "    def get_news_genexpr(self, keyword: str, maxidx: int = -1):\n",
    "        self.search(keyword)\n",
    "        page_count = self.get_page_count()\n",
    "        total_pages = page_count // self.PER_PAGE\n",
    "        if total_pages > maxidx > 0:\n",
    "            total_pages = maxidx\n",
    "        for subidx in range(1, total_pages + 1):\n",
    "            yield from self.get_news_subidx(keyword, subidx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.321175Z",
     "end_time": "2023-04-19T11:30:26.346109Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# searchExample = KookbangPageManager(page)\n",
    "# for i in searchExample.get_news_genexpr('USV'):\n",
    "#     print(i)\n",
    "\n",
    "#searchExampleChosun = ChosunMillitaryPageManager(page)\n",
    "#elems = searchExampleChosun.get_news_genexpr('무인', 3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.343118Z",
     "end_time": "2023-04-19T11:30:26.360072Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import aiohttp\n",
    "class AsyncJupyterChecker:\n",
    "    # wrapper class that wraps run_until_complete\n",
    "    # run_until_complete is not allowed in jupyter notebook. This class will check if it is in jupyter notebook\n",
    "    def __init__(self, loop: asyncio.AbstractEventLoop):\n",
    "        self.loop = loop\n",
    "\n",
    "    def run_until_complete(self, coroutine):\n",
    "        if self.loop.is_closed():\n",
    "            self.loop = asyncio.new_event_loop()\n",
    "        if AsyncJupyterChecker.is_jupyter():\n",
    "            task = self.loop.create_task(coroutine)\n",
    "            self.loop.run_until_complete(task)\n",
    "            return task.result()\n",
    "        else:\n",
    "            return self.loop.run_until_complete(coroutine)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_jupyter():\n",
    "        try:\n",
    "            import IPython\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        if item == 'run_until_complete':\n",
    "            return self.run_until_complete\n",
    "        return getattr(self.loop, item)\n",
    "\n",
    "class NewsData:  # (date, title, content, url)\n",
    "    def __init__(self, date: str, title: str, content: str, url: str):\n",
    "        self.date = date\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.url = url\n",
    "\n",
    "    # Accepts csv writer object, writes data to csv\n",
    "    def write_to_csv(self, csv_writer: csv.writer):\n",
    "        csv_writer.writerow([self.date, self.title, self.content, self.url])\n",
    "\n",
    "class PageParser:\n",
    "    # url -> returns NewsData\n",
    "    # static semaphores\n",
    "    ASYNC_LOOP = AsyncJupyterChecker(asyncio.get_event_loop())  # we will use this loop for async\n",
    "    semaphores = asyncio.Semaphore(10)  # only allow 10 concurrent requests\n",
    "    def __init__(self, url):\n",
    "        if PageParser.ASYNC_LOOP.is_closed():\n",
    "            PageParser.ASYNC_LOOP = AsyncJupyterChecker(asyncio.new_event_loop())\n",
    "        # get date and index from url\n",
    "        self.url = url\n",
    "        # url may be https://kookbang.dema.mil.kr/newsWeb/20230411/4/ATCE_CTGR_0010010000/view.do\n",
    "        # then extract 20230411, 4\n",
    "        self.content = None\n",
    "\n",
    "    def parse(self, text: str) -> NewsData | None:\n",
    "        pass\n",
    "\n",
    "    async def get_html(self) -> str:\n",
    "        async with self.semaphores:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(self.url) as response:\n",
    "                    if response.status != 200:\n",
    "                        raise ConnectionError('Error')\n",
    "                    return await response.text()\n",
    "\n",
    "    async def get_content(self) -> NewsData:\n",
    "        text = await self.get_html()\n",
    "        return self.parse(text)\n",
    "\n",
    "    def get_content_sync(self) -> NewsData:\n",
    "        return self.ASYNC_LOOP.run_until_complete(self.get_content())\n",
    "\n",
    "    @classmethod\n",
    "    def get_content_sync_from_url(cls, url: str) -> NewsData:\n",
    "        return cls(url).get_content_sync()\n",
    "\n",
    "    @classmethod\n",
    "    def get_content_sync_from_urls(cls, urls: [str]) -> [NewsData]:\n",
    "        for url in urls:\n",
    "            yield cls.get_content_sync_from_url(url)\n",
    "\n",
    "    @classmethod\n",
    "    async def get_content_async_from_urls(cls, urls: [str]) -> [NewsData]:\n",
    "        result = []\n",
    "        for contents in asyncio.as_completed([cls(url).get_content() for url in urls]):\n",
    "            result.append(await contents)\n",
    "        return result\n",
    "\n",
    "    @classmethod\n",
    "    def get_contents_from_urls(cls, urls: [str]) -> [NewsData]:\n",
    "        generator = cls.get_content_async_from_urls(urls)\n",
    "        result = cls.ASYNC_LOOP.run_until_complete(generator)\n",
    "        return result\n",
    "\n",
    "class KookbangPageParser(PageParser):\n",
    "    \"\"\"\n",
    "    Parses kookbang page\n",
    "    \"\"\"\n",
    "    def __init__(self, url):\n",
    "        super().__init__(url)\n",
    "        self.date = url.split('/')[4]\n",
    "        self.index = url.split('/')[5]\n",
    "\n",
    "    def parse(self, text: str) -> NewsData | None:\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        # title : <meta property=\"og:title\" content=\"$content\">\n",
    "        # content : <meta property=\"og:description\" content=\"$content\">\n",
    "        # we don't modify date even if there were fixes for content\n",
    "        title = soup.find('meta', property='og:title')\n",
    "        if title is None:\n",
    "            print(f'Error parsing {self.url}')\n",
    "            return None\n",
    "        title = title['content']\n",
    "        contents = soup.find_all('meta', property='og:description')\n",
    "        if contents is None:\n",
    "            merged_string = \"\"\n",
    "        else:\n",
    "            merged_string = \"\"\n",
    "            for content in contents:\n",
    "                merged_string += content['content']\n",
    "        return NewsData(self.date, title, merged_string, self.url)\n",
    "\n",
    "class ChosunPageParser(PageParser):\n",
    "    \"\"\"\n",
    "    Parses chosun page\n",
    "    \"\"\"\n",
    "    def __init__(self, url):\n",
    "        super().__init__(url)\n",
    "        self.date = None\n",
    "\n",
    "    def parse(self, text: str) -> NewsData | None:\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        # title : <div class=\"conSubject\">$content</div>\n",
    "        # content : #container-area > div.area.subpage > div.news_zone_01 > div.news_zone_01_01 > div.board_detail > div.board_body > div:nth-child(4)\n",
    "        title = soup.find('div', class_='conSubject')\n",
    "\n",
    "        if title is None:\n",
    "            print(f'Error parsing {self.url}')\n",
    "            return None\n",
    "        self.date = soup.find('div', class_='wdate')\n",
    "        if self.date is None:\n",
    "            print(f'Error parsing {self.url} (date)')\n",
    "            return None\n",
    "        title = title.get_text(strip=True)\n",
    "        self.date = self.date.get_text(strip=True)\n",
    "        # date = 입력 $date\n",
    "        self.date = self.date.split(' ')[1]\n",
    "        body = soup.find('div', class_='board_body')\n",
    "        if body is None:\n",
    "            merged_string = \"\"\n",
    "        else:\n",
    "            merged_string = \"\"\n",
    "            for contents in body.contents:\n",
    "                text = contents.get_text(strip=True)\n",
    "                text = text.replace(\",\", \"\")\n",
    "                if 'https://' in text or '대표 이미지' in text:\n",
    "                    break\n",
    "                if text not in ['\\n', '', ' ', '\\t']:\n",
    "                    merged_string += text\n",
    "        return NewsData(self.date, title, merged_string, self.url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.372040Z",
     "end_time": "2023-04-19T11:30:26.421908Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.423905Z",
     "end_time": "2023-04-19T11:30:26.434873Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# example url\n",
    "example_url = 'https://kookbang.dema.mil.kr/newsWeb/20230417/2/ATCE_CTGR_0010040000/view.do'\n",
    "# usage -> PageParser(url).get_content_sync()\n",
    "# test_parser = PageParser(example_url)\n",
    "# content = test_parser.get_content_sync()\n",
    "# print(content.content)\n",
    "def crawl_kookbang(kwd, max_page:int = -1):\n",
    "    with open(f'kookbang{kwd}.csv', 'w') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(['date', 'title', 'content', 'url'])\n",
    "        pageManager = KookbangPageManager(page)\n",
    "        for content in KookbangPageParser.get_contents_from_urls(pageManager.get_news_genexpr(kwd, max_page)):\n",
    "            if content is not None:\n",
    "                content.write_to_csv(csv_writer)\n",
    "\n",
    "def crawl_chosun(kwd, max_page:int = -1):\n",
    "    with open(f'chosun{kwd}.csv', 'w', encoding='utf-8') as f:\n",
    "        # use delimiter = '\\t' for tab separated\n",
    "        csv_writer = csv.writer(f, dialect='excel')\n",
    "        csv_writer.writerow(['date', 'title', 'content', 'url'])\n",
    "        pageManager = ChosunMillitaryPageManager(page)\n",
    "        for content in ChosunPageParser.get_contents_from_urls(pageManager.get_news_genexpr(kwd, max_page)):\n",
    "            if content is not None:\n",
    "                content.write_to_csv(csv_writer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.436870Z",
     "end_time": "2023-04-19T11:31:01.585381Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crawl_chosun('USV') # searches USV and saves as USV.csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crawl_kookbang('USV') # searches USV and saves as USV.csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:31:01.585381Z",
     "end_time": "2023-04-19T11:31:01.600997Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
