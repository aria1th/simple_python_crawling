{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:22.661568Z",
     "end_time": "2023-04-19T11:30:22.733569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tested in Jupyter.\n",
    "\n",
    "!pip install selenium\n",
    "!pip install nest_asyncio\n",
    "!pip install tqdm\n",
    "!pip install aiohttp\n",
    "!pip install bs4\n",
    "\n",
    "# imports and global variables\n",
    "import selenium as se\n",
    "import csv\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    def tqdm(x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "SEARCH_BASE_LINK = r'https://kookbang.dema.mil.kr/newsWeb/search.do'\n",
    "# Categories to allow\n",
    "CATEGORIES = {\n",
    "    '국방', '기획연재', '무기백과'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:22.677191Z",
     "end_time": "2023-04-19T11:30:22.733569Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "# using selenium to get the page source\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "page = webdriver.Chrome()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:22.692814Z",
     "end_time": "2023-04-19T11:30:26.282279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# Debug purpose. Everything will be defined as function in the future"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.237929Z",
     "end_time": "2023-04-19T11:30:26.282279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.249896Z",
     "end_time": "2023-04-19T11:30:26.283278Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.268847Z",
     "end_time": "2023-04-19T11:30:26.283278Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "# Standard method to inspect, use F12 -> select element -> copy XPATH.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.286274Z",
     "end_time": "2023-04-19T11:30:26.295244Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "class PageManager:\n",
    "    def __init__(self, page: WebDriver):\n",
    "        self.page = page\n",
    "\n",
    "    def get_element_by_xpath(self, xpath):\n",
    "        return self.page.find_element('xpath', xpath)\n",
    "\n",
    "    def get_element_and_click(self, xpath):\n",
    "        elem = self.get_element_by_xpath(xpath)\n",
    "        elem.click()\n",
    "        return elem\n",
    "\n",
    "    def get_element_and_send_keys(self, xpath, keys):\n",
    "        elem = self.get_element_by_xpath(xpath)\n",
    "        elem.send_keys(keys)\n",
    "        return elem\n",
    "\n",
    "    def get_element_by_selector(self, selector):\n",
    "        try:\n",
    "            return self.page.find_element('css selector', selector)\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "\n",
    "class KookbangPageManager(PageManager):\n",
    "\n",
    "    WAIT_TIME = 1\n",
    "    def __init__(self, page: WebDriver):\n",
    "        super().__init__(page)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.page.get(SEARCH_BASE_LINK)\n",
    "\n",
    "    def search(self, keyword: str):\n",
    "        #  input the search word, find #kwd\n",
    "        self.get_element_and_send_keys('//*[@id=\"kwd\"]', keyword)\n",
    "        self.get_element_and_click('//*[@id=\"container\"]/div[2]/div/form/div/div[2]/button')\n",
    "        self.page.implicitly_wait(self.WAIT_TIME)\n",
    "\n",
    "    def find_category_elems(self) -> [WebElement]:\n",
    "        for i in range(9):\n",
    "            selector = f'#container > div.full_search_box > div > ul > li:nth-child({i}) > a'\n",
    "            elem = self.get_element_by_selector(selector)\n",
    "            if elem is not None:\n",
    "                if elem.text in CATEGORIES:\n",
    "                    yield elem\n",
    "    def get_list_of_news_category(self):\n",
    "        # get next page\n",
    "        # 3 -> 1 page, 4 -> 2 page, ...\n",
    "        # if title is \"다음페이지\" then stop\n",
    "        for pages in range(100):\n",
    "            if pages != 0:\n",
    "                page_selector = f\"#container > div.full_search_box > div > div.pagination > a:nth-child({pages + 3})\"\n",
    "                # starts from 2 page, until we get to the last page\n",
    "                page_elem = self.get_element_by_selector(page_selector)\n",
    "                if page_elem is None or page_elem.get_attribute('title') == '다음페이지':\n",
    "                    break\n",
    "                page_elem.click()\n",
    "                self.page.implicitly_wait(self.WAIT_TIME)\n",
    "            for idx in range(16): # single page may have 15 news\n",
    "                selector = f'#container > div.full_search_box > div > div.box > ul > li:nth-child({idx}) > a'\n",
    "                elem = self.get_element_by_selector(selector)\n",
    "                if elem is None:\n",
    "                    continue\n",
    "                yield elem\n",
    "        #container > div.full_search_box > div > div.pagination > a:nth-child(4)\n",
    "\n",
    "    def get_news_genexpr(self, keyword : str):\n",
    "        self.reset()\n",
    "        self.search(keyword)\n",
    "        for categoryClickButtons in self.find_category_elems():\n",
    "            categoryClickButtons.click()\n",
    "            self.page.implicitly_wait(self.WAIT_TIME)\n",
    "            for news in self.get_list_of_news_category():\n",
    "                yield news.get_attribute('href')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.321175Z",
     "end_time": "2023-04-19T11:30:26.346109Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "# searchExample = KookbangPageManager(page)\n",
    "# for i in searchExample.get_news_genexpr('USV'):\n",
    "#     print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.343118Z",
     "end_time": "2023-04-19T11:30:26.360072Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import aiohttp\n",
    "class AsyncJupyterChecker:\n",
    "    # wrapper class that wraps run_until_complete\n",
    "    # run_until_complete is not allowed in jupyter notebook. This class will check if it is in jupyter notebook\n",
    "    def __init__(self, loop: asyncio.AbstractEventLoop):\n",
    "        self.loop = loop\n",
    "\n",
    "    def run_until_complete(self, coroutine):\n",
    "        if self.loop.is_closed():\n",
    "            self.loop = asyncio.new_event_loop()\n",
    "        if AsyncJupyterChecker.is_jupyter():\n",
    "            task = self.loop.create_task(coroutine)\n",
    "            self.loop.run_until_complete(task)\n",
    "            return task.result()\n",
    "        else:\n",
    "            return self.loop.run_until_complete(coroutine)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_jupyter():\n",
    "        try:\n",
    "            import IPython\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self.loop, item)\n",
    "\n",
    "class NewsData:  # (date, title, content, url)\n",
    "    def __init__(self, date: str, title: str, content: str, url: str):\n",
    "        self.date = date\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.url = url\n",
    "\n",
    "    # Accepts csv writer object, writes data to csv\n",
    "    def write_to_csv(self, csv_writer: csv.writer):\n",
    "        csv_writer.writerow([self.date, self.title, self.content, self.url])\n",
    "\n",
    "class PageParser:\n",
    "    # url -> returns NewsData\n",
    "    # static semaphores\n",
    "    ASYNC_LOOP = AsyncJupyterChecker(asyncio.get_event_loop())  # we will use this loop for async\n",
    "    semaphores = asyncio.Semaphore(10)  # only allow 10 concurrent requests\n",
    "    def __init__(self, url):\n",
    "        if PageParser.ASYNC_LOOP.is_closed():\n",
    "            PageParser.ASYNC_LOOP = AsyncJupyterChecker(asyncio.new_event_loop())\n",
    "        # get date and index from url\n",
    "        self.url = url\n",
    "        # url may be https://kookbang.dema.mil.kr/newsWeb/20230411/4/ATCE_CTGR_0010010000/view.do\n",
    "        # then extract 20230411, 4\n",
    "        self.date = url.split('/')[4]\n",
    "        self.index = url.split('/')[5]\n",
    "        self.content: NewsData|None = None\n",
    "\n",
    "    def parse(self, text: str) -> NewsData | None:\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        # title : <meta property=\"og:title\" content=\"$content\">\n",
    "        # content : <meta property=\"og:description\" content=\"$content\">\n",
    "        # we don't modify date even if there were fixes for content\n",
    "        title = soup.find('meta', property='og:title')\n",
    "        if title is None:\n",
    "            print(f'Error parsing {self.url}')\n",
    "            return None\n",
    "        title = title['content']\n",
    "        contents = soup.find_all('meta', property='og:description')\n",
    "        if contents is None:\n",
    "            merged_string = \"\"\n",
    "        else:\n",
    "            merged_string = \"\"\n",
    "            for content in contents:\n",
    "                merged_string += content['content']\n",
    "        return NewsData(self.date, title, merged_string, self.url)\n",
    "\n",
    "    async def get_html(self) -> str:\n",
    "        async with PageParser.semaphores:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(self.url) as response:\n",
    "                    if response.status != 200:\n",
    "                        raise ConnectionError('Error')\n",
    "                    return await response.text()\n",
    "\n",
    "    async def get_content(self) -> NewsData:\n",
    "        text = await self.get_html()\n",
    "        return self.parse(text)\n",
    "\n",
    "    def get_content_sync(self) -> NewsData:\n",
    "        return PageParser.ASYNC_LOOP.run_until_complete(self.get_content())\n",
    "\n",
    "    @staticmethod\n",
    "    def get_content_sync_from_url(url: str) -> NewsData:\n",
    "        return PageParser(url).get_content_sync()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_content_sync_from_urls(urls: [str]) -> [NewsData]:\n",
    "        for url in urls:\n",
    "            yield PageParser.get_content_sync_from_url(url)\n",
    "\n",
    "    @staticmethod\n",
    "    async def get_content_async_from_urls(urls: [str]) -> [NewsData]:\n",
    "        result = []\n",
    "        for contents in asyncio.as_completed([PageParser(url).get_content() for url in urls]):\n",
    "            result.append(await contents)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def get_contents_from_urls(urls: [str]) -> [NewsData]:\n",
    "        generator = PageParser.get_content_async_from_urls(urls)\n",
    "        result = PageParser.ASYNC_LOOP.run_until_complete(generator)\n",
    "        return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.372040Z",
     "end_time": "2023-04-19T11:30:26.421908Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.423905Z",
     "end_time": "2023-04-19T11:30:26.434873Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing https://kookbang.dema.mil.kr/newsWeb/20191023/2/ATCE_CTGR_0010020002/view.do\n"
     ]
    }
   ],
   "source": [
    "# example url\n",
    "example_url = 'https://kookbang.dema.mil.kr/newsWeb/20230417/2/ATCE_CTGR_0010040000/view.do'\n",
    "# usage -> PageParser(url).get_content_sync()\n",
    "# test_parser = PageParser(example_url)\n",
    "# content = test_parser.get_content_sync()\n",
    "# print(content.content)\n",
    "def crawl_kookbang(kwd):\n",
    "    with open(f'{kwd}.csv', 'w') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(['date', 'title', 'content', 'url'])\n",
    "        pageManager = KookbangPageManager(page)\n",
    "        for content in PageParser.get_contents_from_urls(pageManager.get_news_genexpr(kwd)):\n",
    "            if content is not None:\n",
    "                content.write_to_csv(csv_writer)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:30:26.436870Z",
     "end_time": "2023-04-19T11:31:01.585381Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "crawl_kookbang('USV') # searches USV and saves as USV.csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:31:01.585381Z",
     "end_time": "2023-04-19T11:31:01.600997Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
