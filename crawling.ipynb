{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-18T18:15:37.226099Z",
     "end_time": "2023-05-18T18:16:15.510219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (4.8.3)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (22.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\박상현\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (3.8.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from yarl<2.0,>=1.0->aiohttp) (3.4)\n",
      "Requirement already satisfied: bs4 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from bs4) (4.12.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\박상현\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->bs4) (2.4)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import bs4\n",
    "# Tested in Jupyter.\n",
    "\n",
    "!pip install selenium\n",
    "!pip install nest_asyncio\n",
    "!pip install tqdm\n",
    "!pip install aiohttp\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import aiohttp\n",
    "import csv\n",
    "import selenium as se\n",
    "import csv\n",
    "import os\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    def tqdm(x):\n",
    "        return x\n",
    "# using selenium to get the page source\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "import nest_asyncio"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-18T18:16:15.525114Z",
     "end_time": "2023-05-18T18:16:16.494039Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "SEARCH_BASE_LINK = r'https://kookbang.dema.mil.kr/newsWeb/search.do'\n",
    "# Categories to allow\n",
    "CATEGORIES = {\n",
    "    '국방', '기획연재', '무기백과'\n",
    "}\n",
    "CHOSUN_SEARCH_BASE_LINK = 'https://www.chosun.com/nsearch/?query={kwd}&opt_chk=true&sort=1&siteid=bemil%2Cbemil_news'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-18T18:16:16.494039Z",
     "end_time": "2023-05-18T18:16:16.509629Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "try:\n",
    "    page = webdriver.Chrome()\n",
    "except:\n",
    "    # Locate the chromedriver.exe in same directory\n",
    "    page = webdriver.Chrome('chromedriver.exe')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-18T18:16:16.509629Z",
     "end_time": "2023-05-18T18:16:20.771449Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Standard method to inspect, use F12 -> select element -> copy XPATH.\n",
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-18T18:16:20.657328Z",
     "end_time": "2023-05-18T18:16:20.795391Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "class PageManager:\n",
    "    \"\"\"\n",
    "    Base page manager.\n",
    "    Extended class implements search(kwd) and get_result_genexpr(kwd)-like methods.\n",
    "    Main purpose is to get generator expression of links that can be parsed by BeautifulSoup.\n",
    "    \"\"\"\n",
    "    def __init__(self, driver: WebDriver):\n",
    "        \"\"\"\n",
    "        :type driver: WebDriver\n",
    "        :param driver: Chrome driver, or 'process' that will be controlled by agent\n",
    "        \"\"\"\n",
    "        self.page = driver\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        If Static SEARCH_BASE_LINK is set, set the page to BASE_LINK.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'SEARCH_BASE_LINK'):\n",
    "            self.page.get(self.SEARCH_BASE_LINK)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def get_element_by_xpath(self, xpath):\n",
    "        \"\"\"\n",
    "        Finds element by xpath\n",
    "        :param xpath: str(raw)\n",
    "        :return: element\n",
    "        \"\"\"\n",
    "        return self.page.find_element('xpath', xpath)\n",
    "\n",
    "    def get_element_and_click(self, xpath):\n",
    "        \"\"\"\n",
    "        Finds and clicks element by xpath\n",
    "        :param xpath: str(raw)\n",
    "        :return: element\n",
    "        \"\"\"\n",
    "        elem = self.get_element_by_xpath(xpath)\n",
    "        elem.click()\n",
    "        return elem\n",
    "\n",
    "    def get_element_and_send_keys(self, xpath, keys):\n",
    "        \"\"\"\n",
    "        Finds and sends keys to element by xpath, mainly search\n",
    "        :param xpath: str(raw)\n",
    "        :param keys: str(raw)\n",
    "        :return: element\n",
    "        \"\"\"\n",
    "        elem = self.get_element_by_xpath(xpath)\n",
    "        elem.send_keys(keys)\n",
    "        return elem\n",
    "\n",
    "    def get_element_by_selector(self, selector):\n",
    "        \"\"\"\n",
    "        Using CSS selector, find element.\n",
    "        :param selector: str(raw)\n",
    "        :return: element\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.page.find_element('css selector', selector)\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "\n",
    "class KookbangPageManager(PageManager):\n",
    "    WAIT_TIME = 1\n",
    "    def __init__(self, driver: WebDriver):\n",
    "        super().__init__(driver)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.page.get(SEARCH_BASE_LINK)\n",
    "\n",
    "    def search(self, keyword: str):\n",
    "        #  input the search word, find #kwd\n",
    "        self.get_element_and_send_keys('//*[@id=\"kwd\"]', keyword)\n",
    "        self.get_element_and_click('//*[@id=\"container\"]/div[2]/div/form/div/div[2]/button')\n",
    "        self.page.implicitly_wait(self.WAIT_TIME)\n",
    "\n",
    "    def find_category_elems(self) -> [WebElement]:\n",
    "        for i in range(9):\n",
    "            selector = f'#container > div.full_search_box > div > ul > li:nth-child({i}) > a'\n",
    "            elem = self.get_element_by_selector(selector)\n",
    "            if elem is not None:\n",
    "                if elem.text in CATEGORIES:\n",
    "                    yield elem\n",
    "    def get_list_of_news_category(self, maxidx:int = -1):\n",
    "        # get next page\n",
    "        # 3 -> 1 page, 4 -> 2 page, ...\n",
    "        # if title is \"다음페이지\" then stop\n",
    "        maxidx = maxidx if maxidx != -1 else 100\n",
    "        for pages in range(maxidx):\n",
    "            if pages != 0:\n",
    "                page_selector = f\"#container > div.full_search_box > div > div.pagination > a:nth-child({pages + 3})\"\n",
    "                # starts from 2 page, until we get to the last page\n",
    "                page_elem = self.get_element_by_selector(page_selector)\n",
    "                if page_elem is None or page_elem.get_attribute('title') == '다음페이지':\n",
    "                    break\n",
    "                page_elem.click()\n",
    "                self.page.implicitly_wait(self.WAIT_TIME)\n",
    "            for idx in range(16): # single page may have 15 news\n",
    "                selector = f'#container > div.full_search_box > div > div.box > ul > li:nth-child({idx}) > a'\n",
    "                elem = self.get_element_by_selector(selector)\n",
    "                if elem is None:\n",
    "                    continue\n",
    "                yield elem\n",
    "        #container > div.full_search_box > div > div.pagination > a:nth-child(4)\n",
    "\n",
    "    def get_news_genexpr(self, keyword : str, maxidx:int = -1):\n",
    "        \"\"\"\n",
    "        Get news generator expression.\n",
    "        :param keyword: Keyword to search\n",
    "        :param maxidx: Maximum number of news to return. -1 for all.\n",
    "        :return: Generator object that returns news link.\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        self.search(keyword)\n",
    "        for categoryClickButtons in self.find_category_elems():\n",
    "            categoryClickButtons.click()\n",
    "            self.page.implicitly_wait(self.WAIT_TIME)\n",
    "            for news in self.get_list_of_news_category(maxidx):\n",
    "                yield news.get_attribute('href')\n",
    "\n",
    "class ChosunMillitaryPageManager(PageManager):\n",
    "    WAIT_TIME = 3\n",
    "    PER_PAGE = 10  # 10 news per page.\n",
    "    BANNED_BBSID = {\n",
    "        10044,\n",
    "        10040,\n",
    "        10129,\n",
    "        10046,\n",
    "        10037\n",
    "    }\n",
    "\n",
    "    def __init__(self, driver: WebDriver):\n",
    "        super().__init__(driver)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def search(self, keyword: str):\n",
    "        #  input the search word, find #kwd\n",
    "        self.page.get(CHOSUN_SEARCH_BASE_LINK.format(kwd=keyword))\n",
    "        self.page.implicitly_wait(self.WAIT_TIME)\n",
    "        self.page.maximize_window()\n",
    "\n",
    "    def get_page_count(self):\n",
    "        elem = self.get_element_by_selector('#main > div.search-option > div.flex.flex--justify-space-between.flex--align-items-center.box--pad-bottom-sm.box--border.box--border-horizontal.box--border-horizontal-bottom > div:nth-child(1) > p')\n",
    "        text = elem.text\n",
    "        m = re.search(r'(\\d+)건', text)\n",
    "        return int(m.group(1))\n",
    "\n",
    "    def get_page_for_kwd(self, kwd: str, subidx: int):\n",
    "        if subidx == 0:\n",
    "            return\n",
    "        sub_search_page = f'https://www.chosun.com/nsearch/?query={kwd}&page={subidx}&siteid=bemil,bemil_news&sort=1'\n",
    "        self.page.get(sub_search_page)\n",
    "        self.page.implicitly_wait(self.WAIT_TIME)\n",
    "\n",
    "    def get_news_subidx(self, kwd: str, subidx: int):\n",
    "        self.get_page_for_kwd(kwd, subidx)\n",
    "        for idx in range(1, 11):\n",
    "            xpath = f'//*[@id=\"main\"]/div[4]/div[{idx}]/div/div[1]/div[2]/div[1]/div/a'\n",
    "            elem = self.get_element_by_xpath(xpath)\n",
    "            if elem is None:\n",
    "                pass\n",
    "            href = elem.get_attribute('href')\n",
    "            m = re.search(r'bbs_id=(\\d+)', href)\n",
    "            bbsid = int(m.group(1))\n",
    "            if bbsid in self.BANNED_BBSID:\n",
    "                continue\n",
    "            yield href\n",
    "\n",
    "    def get_news_genexpr(self, keyword: str, maxidx: int = -1):\n",
    "        self.search(keyword)\n",
    "        page_count = self.get_page_count()\n",
    "        total_pages = page_count // self.PER_PAGE\n",
    "        if total_pages > maxidx > 0:\n",
    "            total_pages = maxidx\n",
    "        for subidx in range(1, total_pages + 1):\n",
    "            yield from self.get_news_subidx(keyword, subidx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-18T18:16:20.686605Z",
     "end_time": "2023-05-18T18:16:20.866890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import requests\n",
    "# We can ignore empty fields\n",
    "# just join at http://www.riss.kr/search/Search.do? with &\n",
    "# isDetailSearch : N\n",
    "# searchGubun : true\n",
    "# viewYn : OP\n",
    "# strQuery : $keyword.replace(' ', '+')\n",
    "# order : /DESC\n",
    "# onHanja : false\n",
    "# strSort : RANK\n",
    "# iStartCount : $pageScale * ($pageNumber - 1)\n",
    "# fsearchMethod : search\n",
    "# sflag : 1\n",
    "# isFDetailSearch : N\n",
    "# pageNumber : $page\n",
    "# resultKeyword : $keyword.replace(' ', '+')\n",
    "# icate : bib_t\n",
    "# colName : bib_t | re_a_kor - if academic, use bib_t\n",
    "# pageScale : $pageScale oneOf(10, 100)\n",
    "# isTab : Y\n",
    "# query : $keyword.replace(' ', '+')\n",
    "\n",
    "class WriteableRecord:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        usage : WriteableRecord(*[0,1,2,3]).write_to_csv(csv_writer)\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        self.listed_args = args\n",
    "        self.dicted_args = kwargs\n",
    "    def write_to_csv(self, csv_writer: csv.writer):\n",
    "        csv_writer.writerow(*list(self.listed_args) + [self.dicted_args[arg] for arg in self.dicted_args])\n",
    "\n",
    "class RISSSearchManager(PageManager):\n",
    "    SEARCH_BASE_LINK = r'http://www.riss.kr/index.do'\n",
    "    WAIT_TIME = 3\n",
    "    \"\"\"\n",
    "    RISS Search Manager\n",
    "    We do not use selenium here\n",
    "    \"\"\"\n",
    "    def __init__(self, driver: WebDriver):\n",
    "        super().__init__(driver)\n",
    "\n",
    "    def wait(self):\n",
    "        self.page.implicitly_wait(RISSSearchManager.WAIT_TIME)\n",
    "\n",
    "    def get_page_for_search(self, keyword:str, is_academic:bool=False, page_scale:int=10, page_idx:int=1):\n",
    "        # use beautiful soup to parse the page, we will use session with urllib\n",
    "        _query = keyword.replace(' ', '+')\n",
    "        _start_count = page_scale * (page_idx - 1)\n",
    "        _col_name = 'bib_t' if is_academic else 're_a_kor'\n",
    "        _page_scale = page_scale\n",
    "        _page_number = page_idx\n",
    "        search_page = fr'http://www.riss.kr/search/Search.do?isDetailSearch=N&searchGubun=true&viewYn=OP&query={_query}&queryText=&iStartCount={_start_count}&iGroupView=5&icate=all&colName={_col_name}&exQuery=&exQueryText=&order=%2FDESC&onHanja=false&strSort=RANK&pageScale={_page_scale}&orderBy=&fsearchMethod=search&isFDetailSearch=N&sflag=1&searchQuery={_query}&fsearchSort=&fsearchOrder=&limiterList=&limiterListText=&facetList=&facetListText=&fsearchDB=&resultKeyword={_query}&pageNumber={_page_number}&p_year1=&p_year2=&dorg_storage=&mat_type=&mat_subtype=&fulltext_kind=&t_gubun=&learning_type=&language_code=&ccl_code=&language=&inside_outside=&fric_yn=&image_yn=&regnm=&gubun=&kdc=&ttsUseYn='\n",
    "        request = requests.get(search_page)\n",
    "        if request.status_code != 200:\n",
    "            raise Exception('Request failed, status code : ' + str(request.status_code) + ', url : ' + search_page)\n",
    "        return BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "    def find_result_number(self, soup:BeautifulSoup):\n",
    "        # find number\n",
    "        parse = soup.findAll('span', class_='num')\n",
    "        for p in parse:\n",
    "            if p.getText().replace(',', '').isdigit():\n",
    "                return int(p.getText().replace(',', ''))\n",
    "        return 0\n",
    "\n",
    "    def search_and_parse(self, keyword:str, is_academic:bool=False, page_scale:int=10, max_results:int=1000):\n",
    "        soup = self.get_page_for_search(keyword, is_academic, page_scale, 1)\n",
    "        total_expected_pages = self.get_page_count(soup, page_scale)\n",
    "        total_expected_pages = min(total_expected_pages, max_results // page_scale)\n",
    "        collected = []\n",
    "        for page_idx in tqdm(range(1, total_expected_pages + 1)):\n",
    "            soup = self.get_page_for_search(keyword, is_academic, page_scale, page_idx)\n",
    "            for idx in range(page_scale):\n",
    "                collected.append(self.parse_result_list(soup, idx))\n",
    "                if len(collected) >= max_results:\n",
    "                    return collected\n",
    "        return collected\n",
    "\n",
    "    def search_and_parse_genexpr(self, keyword:str, is_academic:bool=False, page_scale:int=10, max_results:int=1000):\n",
    "        soup = self.get_page_for_search(keyword, is_academic, page_scale, 1)\n",
    "        total_expected_pages = self.get_page_count(soup, page_scale)\n",
    "        total_expected_pages = min(total_expected_pages, max_results // page_scale)\n",
    "        _i=0\n",
    "        for page_idx in tqdm(range(1, total_expected_pages + 1)):\n",
    "            soup = self.get_page_for_search(keyword, is_academic, page_scale, page_idx)\n",
    "            for idx in range(page_scale):\n",
    "                yield self.parse_result_list(soup, idx)\n",
    "                _i+=1\n",
    "                if _i >= max_results:\n",
    "                    return\n",
    "\n",
    "    def save_search(self, keyword:str, is_academic:bool=False, page_scale:int=10, max_results:int=1000,\n",
    "                    filename:str='result.csv', encoding:str='utf-8'):\n",
    "        if os.path.exists(filename):\n",
    "            raise Exception(f'File {filename} already exists')\n",
    "        with open(filename, 'w', encoding=encoding, newline='') as f:\n",
    "            csv_writer = csv.writer(f)\n",
    "            for _result in self.search_and_parse_genexpr(keyword, is_academic, page_scale, max_results):\n",
    "                WriteableRecord(_result).write_to_csv(csv_writer)\n",
    "\n",
    "    def get_page_count(self, soup:BeautifulSoup, page_scale:int=10):\n",
    "        total_results = self.find_result_number(soup)\n",
    "        return total_results // page_scale + 1 if total_results % page_scale != 0 else total_results // page_scale\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_result_list(result_soup:BeautifulSoup, list_idx:int=0):\n",
    "        result_soup_list = result_soup.find('div', class_='srchResultListW')\n",
    "        lists = [l for l in result_soup_list.findAll('li') if l.find('div', class_='cont') is not None]\n",
    "        parts = lists[list_idx]\n",
    "        # title = find p class=\"title\"\n",
    "        title = parts.find('p', class_='title')\n",
    "        # writer = span class=\"writer\"\n",
    "        writer = parts.find('span', class_='writer')\n",
    "        # year = <span>20xx</span>\n",
    "        year = parts.find('p', class_='etc').findAll('span')[2]\n",
    "        # degree = 3rd\n",
    "        degree = parts.find('p', class_='etc').findAll('span')[3]\n",
    "        # abstract = p class=\"preAbstract\"\n",
    "        abstract = parts.find('p', class_='preAbstract')\n",
    "        collection = title, writer, year, degree, abstract\n",
    "        return [c.getText() for c in collection if c is not None]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-18T18:16:20.804402Z",
     "end_time": "2023-05-18T18:16:21.068709Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 9/147 [00:34<08:52,  3.86s/it]\n"
     ]
    }
   ],
   "source": [
    "result = RISSSearchManager(page).save_search(keyword='간호 AND 안전', is_academic=True, page_scale=10, max_results=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-18T18:16:21.068709Z",
     "end_time": "2023-05-18T18:16:58.049871Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class AsyncJupyterChecker:\n",
    "    # wrapper class that wraps run_until_complete\n",
    "    # run_until_complete is not allowed in jupyter notebook. This class will check if it is in jupyter notebook\n",
    "    def __init__(self, loop: asyncio.AbstractEventLoop):\n",
    "        self.loop = loop\n",
    "\n",
    "    def run_until_complete(self, coroutine):\n",
    "        if self.loop.is_closed():\n",
    "            self.loop = asyncio.new_event_loop()\n",
    "        if AsyncJupyterChecker.is_jupyter():\n",
    "            task = self.loop.create_task(coroutine)\n",
    "            self.loop.run_until_complete(task)\n",
    "            return task.result()\n",
    "        else:\n",
    "            return self.loop.run_until_complete(coroutine)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_jupyter():\n",
    "        try:\n",
    "            import IPython\n",
    "            return True\n",
    "        except ImportError:\n",
    "            return False\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        if item == 'run_until_complete':\n",
    "            return self.run_until_complete\n",
    "        return getattr(self.loop, item)\n",
    "\n",
    "class NewsData:  # (date, title, content, url)\n",
    "    def __init__(self, date: str, title: str, content: str, url: str):\n",
    "        self.date = date\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.url = url\n",
    "\n",
    "    # Accepts csv writer object, writes data to csv\n",
    "    def write_to_csv(self, csv_writer: csv.writer):\n",
    "        csv_writer.writerow([self.date, self.title, self.content, self.url])\n",
    "\n",
    "\n",
    "\n",
    "class PageParser:\n",
    "    # url -> returns NewsData\n",
    "    # static semaphores\n",
    "    ASYNC_LOOP = AsyncJupyterChecker(asyncio.get_event_loop())  # we will use this loop for async\n",
    "    semaphores = asyncio.Semaphore(10)  # only allow 10 concurrent requests\n",
    "    def __init__(self, url):\n",
    "        if PageParser.ASYNC_LOOP.is_closed():\n",
    "            PageParser.ASYNC_LOOP = AsyncJupyterChecker(asyncio.new_event_loop())\n",
    "        # get date and index from url\n",
    "        self.url = url\n",
    "        # url may be https://kookbang.dema.mil.kr/newsWeb/20230411/4/ATCE_CTGR_0010010000/view.do\n",
    "        # then extract 20230411, 4\n",
    "        self.content = None\n",
    "\n",
    "    def parse(self, text: str) -> NewsData | None:\n",
    "        pass\n",
    "\n",
    "    async def get_html(self) -> str:\n",
    "        async with self.semaphores:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(self.url) as response:\n",
    "                    if response.status != 200:\n",
    "                        raise ConnectionError('Error')\n",
    "                    return await response.text()\n",
    "\n",
    "    async def get_content(self) -> NewsData:\n",
    "        text = await self.get_html()\n",
    "        return self.parse(text)\n",
    "\n",
    "    def get_content_sync(self) -> NewsData:\n",
    "        return self.ASYNC_LOOP.run_until_complete(self.get_content())\n",
    "\n",
    "    @classmethod\n",
    "    def get_content_sync_from_url(cls, url: str) -> NewsData:\n",
    "        return cls(url).get_content_sync()\n",
    "\n",
    "    @classmethod\n",
    "    def get_content_sync_from_urls(cls, urls: [str]) -> [NewsData]:\n",
    "        for url in urls:\n",
    "            yield cls.get_content_sync_from_url(url)\n",
    "\n",
    "    @classmethod\n",
    "    async def get_content_async_from_urls(cls, urls: [str]) -> [NewsData]:\n",
    "        result = []\n",
    "        for contents in asyncio.as_completed([cls(url).get_content() for url in urls]):\n",
    "            result.append(await contents)\n",
    "        return result\n",
    "\n",
    "    @classmethod\n",
    "    def get_contents_from_urls(cls, urls: [str]) -> [NewsData]:\n",
    "        generator = cls.get_content_async_from_urls(urls)\n",
    "        result = cls.ASYNC_LOOP.run_until_complete(generator)\n",
    "        return result\n",
    "\n",
    "class KookbangPageParser(PageParser):\n",
    "    \"\"\"\n",
    "    Parses kookbang page\n",
    "    \"\"\"\n",
    "    def __init__(self, url):\n",
    "        super().__init__(url)\n",
    "        self.date = url.split('/')[4]\n",
    "        self.index = url.split('/')[5]\n",
    "\n",
    "    def parse(self, text: str) -> NewsData | None:\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        # title : <meta property=\"og:title\" content=\"$content\">\n",
    "        # content : <meta property=\"og:description\" content=\"$content\">\n",
    "        # we don't modify date even if there were fixes for content\n",
    "        title = soup.find('meta', property='og:title')\n",
    "        if title is None:\n",
    "            print(f'Error parsing {self.url}')\n",
    "            return None\n",
    "        title = title['content']\n",
    "        contents = soup.find_all('meta', property='og:description')\n",
    "        if contents is None:\n",
    "            merged_string = \"\"\n",
    "        else:\n",
    "            merged_string = \"\"\n",
    "            for content in contents:\n",
    "                merged_string += content['content']\n",
    "        return NewsData(self.date, title, merged_string, self.url)\n",
    "\n",
    "class ChosunPageParser(PageParser):\n",
    "    \"\"\"\n",
    "    Parses chosun page\n",
    "    \"\"\"\n",
    "    def __init__(self, url):\n",
    "        super().__init__(url)\n",
    "        self.date = None\n",
    "\n",
    "    def parse(self, text: str) -> NewsData | None:\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        # title : <div class=\"conSubject\">$content</div>\n",
    "        # content : #container-area > div.area.subpage > div.news_zone_01 > div.news_zone_01_01 > div.board_detail > div.board_body > div:nth-child(4)\n",
    "        title = soup.find('div', class_='conSubject')\n",
    "\n",
    "        if title is None:\n",
    "            print(f'Error parsing {self.url}')\n",
    "            return None\n",
    "        self.date = soup.find('div', class_='wdate')\n",
    "        if self.date is None:\n",
    "            print(f'Error parsing {self.url} (date)')\n",
    "            return None\n",
    "        title = title.get_text(strip=True)\n",
    "        self.date = self.date.get_text(strip=True)\n",
    "        # date = 입력 $date\n",
    "        self.date = self.date.split(' ')[1]\n",
    "        body = soup.find('div', class_='board_body')\n",
    "        if body is None:\n",
    "            merged_string = \"\"\n",
    "        else:\n",
    "            merged_string = \"\"\n",
    "            for contents in body.contents:\n",
    "                text = contents.get_text(strip=True)\n",
    "                text = text.replace(\",\", \"\")\n",
    "                if 'https://' in text or '대표 이미지' in text:\n",
    "                    break\n",
    "                if text not in ['\\n', '', ' ', '\\t']:\n",
    "                    merged_string += text\n",
    "        return NewsData(self.date, title, merged_string, self.url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-18T18:16:58.049871Z",
     "end_time": "2023-05-18T18:16:58.143599Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# example url\n",
    "# example_url = 'https://kookbang.dema.mil.kr/newsWeb/20230417/2/ATCE_CTGR_0010040000/view.do'\n",
    "# usage -> PageParser(url).get_content_sync()\n",
    "# test_parser = PageParser(example_url)\n",
    "# content = test_parser.get_content_sync()\n",
    "# print(content.content)\n",
    "def crawl_kookbang(kwd, max_page:int = -1):\n",
    "    with open(f'kookbang{kwd}.csv', 'w', newline='') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(['date', 'title', 'content', 'url'])\n",
    "        page_manager = KookbangPageManager(page)\n",
    "        for content in KookbangPageParser.get_contents_from_urls(page_manager.get_news_genexpr(kwd, max_page)):\n",
    "            if content is not None:\n",
    "                content.write_to_csv(csv_writer)\n",
    "\n",
    "def crawl_chosun(kwd, max_page:int = -1):\n",
    "    with open(f'chosun{kwd}.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "        # use delimiter = '\\t' for tab separated\n",
    "        csv_writer = csv.writer(f, dialect='excel')\n",
    "        csv_writer.writerow(['date', 'title', 'content', 'url'])\n",
    "        page_manager = ChosunMillitaryPageManager(page)\n",
    "        for content in ChosunPageParser.get_contents_from_urls(page_manager.get_news_genexpr(kwd, max_page)):\n",
    "            if content is not None:\n",
    "                content.write_to_csv(csv_writer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-18T18:16:58.143599Z",
     "end_time": "2023-05-18T18:16:58.221987Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#crawl_chosun('USV') # searches USV and saves as USV.csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-18T18:16:58.190746Z",
     "end_time": "2023-05-18T18:16:58.237608Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#crawl_kookbang('USV') # searches USV and saves as USV.csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-18T18:16:58.221987Z",
     "end_time": "2023-05-18T18:16:58.268852Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
